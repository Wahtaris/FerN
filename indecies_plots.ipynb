{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vegetation Index Analysis for Agricultural Parcels\n",
    "\n",
    "*Author: Alessandro Joshua Pierro*\n",
    "\n",
    "*Affiliation: Agrscope*\n",
    "\n",
    "*Date: December 2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Overview\n",
    "\n",
    "This notebook presents a detailed analysis of vegetation indices derived from Sentinel-2 satellite imagery. The indices, including NDVI, NDRE, and EVI, are crucial for monitoring the health and growth of crops over time within specific agricultural parcels.\n",
    "\n",
    "## Objective\n",
    "\n",
    "Our aim is to track the changes in these indices across different parcels, providing insights into crop development stages and potential stress factors affecting crop vigor.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "The approach involves several key steps:\n",
    "\n",
    "1. Preprocessing Sentinel-2 Scenes: This includes resampling and masking clouds, shadows, and snow using the preprocess_sentinel2_scenes function.\n",
    "\n",
    "2. Extracting Spectral Data: The extract_s2data function pulls relevant data from Sentinel-2 imagery within specified time frames and cloud cover thresholds.\n",
    "\n",
    "3. Processing Shapefiles: Aligning satellite data with geographical data of parcels.\n",
    "\n",
    "4. Synthesizing Data: The process_shapefiles_and_extract_sentinel_data function compiles the data to generate time series graphs of the vegetation indices for each parcel.\n",
    "\n",
    "\n",
    "## Kexy Functions\n",
    "\n",
    "preprocess_sentinel2_scenes: Handles the preprocessing of Sentinel-2 scenes.\n",
    "extract_s2_data: Extracts data from Sentinel-2 scenes based on specified parameters.\n",
    "process_shapefiles_and_extract_sentinel_data: Combines shapefile data with extracted satellite data to produce temporal vegetation index graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries and Dependencies\n",
    "\n",
    "Below are the necessary libraries and modules required for the geospatial analysis and visualization. Ensure these are installed in your Python environment before running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from eodal.core.scene import SceneCollection\n",
    "from eodal.core.sensors.sentinel2 import Sentinel2\n",
    "from eodal.mapper.feature import Feature\n",
    "from eodal.mapper.filter import Filter\n",
    "from eodal.mapper.mapper import Mapper, MapperConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set environment variable to enable working with STAC from Planetary Computer\n",
    "from eodal.config import get_settings\n",
    "settings = get_settings()\n",
    "settings.USE_STAC = True\n",
    "\n",
    "\n",
    "def preprocess_sentinel2_scenes(\n",
    "    ds: Sentinel2,\n",
    "    target_resolution: int,\n",
    ") -> Sentinel2:\n",
    "    \"\"\"\n",
    "    Resample Sentinel-2 scenes and mask clouds, shadows, and snow\n",
    "    based on the Scene Classification Layer (SCL).\n",
    "\n",
    "    NOTE:\n",
    "        Depending on your needs, the pre-processing function can be\n",
    "        fully customized using the full power of EOdal and its\n",
    "    interfacing libraries!\n",
    "\n",
    "    :param target_resolution:\n",
    "        spatial target resolution to resample all bands to.\n",
    "    :returns:\n",
    "        resampled, cloud-masked Sentinel-2 scene.\n",
    "    \"\"\"\n",
    "    # resample scene\n",
    "    ds.resample(inplace=True, target_resolution=target_resolution)\n",
    "    # mask clouds, shadows, and snow\n",
    "    ds.mask_clouds_and_shadows(inplace=True, cloud_classes=[3, 8, 9, 10, 11])\n",
    "    return ds\n",
    "\n",
    "def extract_s2_data(\n",
    "        parcel: gpd.GeoDataFrame,\n",
    "        time_start: datetime,\n",
    "        time_end: datetime,\n",
    "        scene_cloud_cover_threshold: float = 80,\n",
    "        feature_cloud_cover_threshold: float = 50,\n",
    "        spatial_resolution: int = 10\n",
    "    ) -> SceneCollection:\n",
    "    \"\"\"\n",
    "    Extracts Sentinel-2 data from the STAC SAT archive for a given area and time period.\n",
    "    Scenes that are too cloudy or contain nodata (blackfill), only, are discarded.\n",
    "\n",
    "    The processing level of the Sentinel-2 data is L2A (surface reflectance factors).\n",
    "\n",
    "    :param parcel:\n",
    "        field parcel geometry (defines the spatial extent to extract)\n",
    "    :param time_start:\n",
    "        start of the time period to extract\n",
    "    :param end_time:\n",
    "        end of the time period to extract\n",
    "    :param scene_cloud_cover_threshold:\n",
    "        scene-wide cloudy pixel percentage (from Sentinel-2 metadata) to filter out scenes\n",
    "        with too high cloud coverage values [0-100%]\n",
    "    :param feature_cloud_cover_threshold:\n",
    "        cloudy pixel percentage [0-100%] on the parcel level. Only if the parcel has a\n",
    "        lower percentual share of cloudy pixels (based on the scene classification layer) than\n",
    "        the threshold specified, the Sentinel-2 observation is kept\n",
    "    :param spatial_resolution:\n",
    "        spatial resolution of the Sentinel-2 data in meters (Def: 10m)\n",
    "    :param resampling_method:\n",
    "        spatial resampling method for those Sentinel-2 bands not available in the target\n",
    "        resolution. Nearest Neighbor by default\n",
    "    :returns:\n",
    "        dictionary with the list of scenes for the field parcel (`feature_scenes`), the\n",
    "        DataFrame of (un)used scenes and the reason for not using plus some basic scene\n",
    "        metadata (`scene_properties`)\n",
    "    \"\"\"\n",
    "    # setup the metadata filters (cloud cover and processing level)\n",
    "    metadata_filters = [\n",
    "        Filter('cloudy_pixel_percentage','<', scene_cloud_cover_threshold),\n",
    "        Filter('processing_level', '==', 'Level-2A')\n",
    "    ]\n",
    "    # setup the spatial feature for extracting data\n",
    "    feature = Feature.from_geoseries(parcel.geometry)\n",
    "    \n",
    "    # set up mapping configs\n",
    "    mapper_configs = MapperConfigs(\n",
    "        collection='sentinel2-msi',\n",
    "        time_start=time_start,\n",
    "        time_end=time_end,\n",
    "        feature=feature,\n",
    "        metadata_filters=metadata_filters\n",
    "    )\n",
    "\n",
    "    # get a new mapper instance. Set sensor to `sentinel2`\n",
    "    mapper = Mapper(mapper_configs)\n",
    "\n",
    "    # query the STAC (looks for available scenes in the selected spatio-temporal range)\n",
    "    mapper.query_scenes()\n",
    "\n",
    "    # get observations (loads the actual Sentinel2 scenes)\n",
    "    # the data is extract for the extent of the parcel\n",
    "    scene_kwargs = {\n",
    "        'scene_constructor': Sentinel2.from_safe,            # this tells the mapper how to read and load the data (i.e., Sentinel-2 scenes)\n",
    "        'scene_constructor_kwargs': {},                      # here you could specify which bands to read\n",
    "        'scene_modifier': preprocess_sentinel2_scenes,       # this tells the mapper about (optional) pre-processing of the loaded scenes (must be a callable)\n",
    "        'scene_modifier_kwargs': {'target_resolution': 10}   # here, you have to specify the value of the arguments the `scene_modifier` requires\n",
    "    }\n",
    "    mapper.load_scenes(scene_kwargs=scene_kwargs)\n",
    "\n",
    "    # loop over available Sentinel-2 scenes stored in mapper.data as a EOdal SceneCollection and check\n",
    "    # for no-data. These scenes will be removed from the SceneCollection\n",
    "    scenes_to_del = []\n",
    "    mapper.metadata['scene_used'] = 'yes'\n",
    "    for scene_id, scene in mapper.data:\n",
    "\n",
    "        # check if scene is blackfilled (nodata); if yes continue\n",
    "        if scene.is_blackfilled:\n",
    "            scenes_to_del.append(scene_id)\n",
    "            mapper.metadata.loc[mapper.metadata.sensing_time.dt.strftime('%Y-%m-%d %H:%M') == scene_id.strftime('%Y-%m-%d %H:%M')[0:16], 'scene_used'] = 'No [blackfill]'\n",
    "            continue\n",
    "\n",
    "        # check cloud coverage (including shadows and snow) of the field parcel\n",
    "        feature_cloud_cover = scene.get_cloudy_pixel_percentage(cloud_classes=[3, 8, 9, 10, 11])\n",
    "\n",
    "        # if the scene is too cloudy, we skip it\n",
    "        if feature_cloud_cover > feature_cloud_cover_threshold:\n",
    "            scenes_to_del.append(scene_id)\n",
    "            mapper.metadata.loc[mapper.metadata.sensing_time.dt.strftime('%Y-%m-%d %H:%M') == scene_id.strftime('%Y-%m-%d %H:%M')[0:16], 'scene_used'] = 'No [clouds]'\n",
    "            continue\n",
    "\n",
    "        # calculate the NDVI, NDRE, EVI\n",
    "        scene.calc_si('NDVI', inplace=True)\n",
    "        scene.calc_si('NDRE', inplace=True)\n",
    "        scene.calc_si('EVI', inplace=True)\n",
    "        \n",
    "\n",
    "    # delete scenes too cloudy or containing only no-data\n",
    "    for scene_id in scenes_to_del:\n",
    "        del mapper.data[scene_id]\n",
    "    \n",
    "    return mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for a cleaner output\n",
    "def suppress_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up the base directory and results directories\n",
    "def setup_directories():\n",
    "    base_dir = Path(os.path.dirname(os.path.realpath(\"__file__\")))\n",
    "    png_dir = base_dir / 'results' / 'parcel_graphs' / 'png'\n",
    "    pdf_dir = base_dir / 'results' / 'parcel_graphs' / 'pdf'\n",
    "    png_dir.mkdir(parents=True, exist_ok=True)\n",
    "    pdf_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return base_dir, png_dir, pdf_dir\n",
    "\n",
    "# Retrieve all subfolders within the base folder\n",
    "def get_subfolders(base_folder):\n",
    "    return [f.path for f in os.scandir(base_folder) if f.is_dir()]\n",
    "\n",
    "# Find all shapefiles within a given subfolder\n",
    "def find_shapefiles_in_subfolder(subfolder):\n",
    "    return [f.path for f in os.scandir(subfolder) if f.name.endswith('.shp')]\n",
    "\n",
    "\n",
    "# Extract satellite data for each parcel\n",
    "def extract_data_for_parcels(parcels_geo_df):\n",
    "    results = []\n",
    "    for idx, parcel in parcels_geo_df.iterrows():\n",
    "        parcel_gdf = gpd.GeoDataFrame([parcel], geometry='geometry', crs=parcels_geo_df.crs)\n",
    "        results.append(extract_s2_data(parcel=parcel_gdf, time_start=datetime(2022, 1, 1), time_end=datetime(2023, 11, 21))) # change date as needed\n",
    "    return results\n",
    "\n",
    "# Plot data for each treatment over time\n",
    "def plot_treatment_data(axes, treatment_data, color, treatment_name):\n",
    "    for idx, ax in enumerate(axes):\n",
    "        ax.plot(treatment_data['date'], treatment_data[idx], label=f'{treatment_name} - {idx}', marker='o', color=color)\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "\n",
    "# Save the plots for each subfolder\n",
    "def save_plots(subfolder_name, fig, png_dir, pdf_dir):\n",
    "    png_path = png_dir / f'Extraction_{subfolder_name}_graph.png'\n",
    "    pdf_path = pdf_dir / f'Extraction_{subfolder_name}_graph.pdf'\n",
    "    fig.savefig(png_path, format='png', bbox_inches='tight')\n",
    "    fig.savefig(pdf_path, format='pdf', bbox_inches='tight')\n",
    "\n",
    "def interpolate_and_smooth_data(subset, index_label, window_size=7):\n",
    "    \"\"\" Interpolate missing data points and apply smoothing for a smoother line. \"\"\"\n",
    "    subset['date'] = pd.to_datetime(subset['date'])\n",
    "    subset = subset.set_index('date')\n",
    "\n",
    "    # Resample the data to a regular interval (e.g., daily)\n",
    "    subset_resampled = subset[index_label].resample('D').mean()\n",
    "\n",
    "    # Interpolate the missing values\n",
    "    subset_interpolated = subset_resampled.interpolate(method='time')\n",
    "\n",
    "    # Apply a rolling window to smooth the data\n",
    "    subset_smoothed = subset_interpolated.rolling(window=window_size, min_periods=1, center=True).mean()\n",
    "\n",
    "    # Reset the index to get 'date' back as a column\n",
    "    subset_smoothed = subset_smoothed.reset_index()\n",
    "    return subset_smoothed\n",
    "\n",
    "\n",
    "def compile_time_series_data(result_objects, parcel_gdf):\n",
    "    all_pixel_ts_list = []\n",
    "    for res_objects in result_objects:\n",
    "            all_pixel_ts_subfolder = pd.DataFrame()\n",
    "\n",
    "            for res in res_objects:\n",
    "                metadata = res.metadata[['product_uri', 'sensing_date', 'scene_used']]\n",
    "                res_complete = res.data.sort()\n",
    "                records = []\n",
    "\n",
    "                for sensing_date, scene in res_complete:\n",
    "                    rec = scene.get_pixels(vector_features=parcel_gdf, band_selection=['NDVI', 'NDRE', 'EVI'])\n",
    "                    rec['date'] = sensing_date.date()\n",
    "                    records.append(rec)\n",
    "\n",
    "                pixel_ts = pd.concat(records)\n",
    "                pixel_ts = pixel_ts.sort_values(by='date')\n",
    "                all_pixel_ts_subfolder = pd.concat([all_pixel_ts_subfolder, pixel_ts])\n",
    "\n",
    "            all_pixel_ts_list.append(all_pixel_ts_subfolder)\n",
    "    return all_pixel_ts_list\n",
    "\n",
    "# Plot graphs for each index\n",
    "def plot_index_graphs(axes, all_pixel_ts, color_mapping):\n",
    "    for traitement, color in color_mapping.items():\n",
    "        subset = all_pixel_ts[all_pixel_ts['Traitement'] == traitement].copy()\n",
    "        for idx, index_label in enumerate(['NDVI', 'NDRE', 'EVI']):\n",
    "            if subset.empty or index_label not in subset.columns:\n",
    "                continue\n",
    "\n",
    "            # Aggregate only the numeric data (index values) by date\n",
    "            subset_aggregated = subset[[index_label, 'date']].groupby('date').mean().reset_index()\n",
    "\n",
    "            # Interpolate and smooth the data\n",
    "            smoothed_data = interpolate_and_smooth_data(subset_aggregated, index_label)\n",
    "\n",
    "            # Plot the smoothed line\n",
    "            axes[idx].plot(smoothed_data['date'], smoothed_data[index_label], label=f'{traitement} - {index_label}', linestyle='-', color=color)\n",
    "            # Plot the original data points on top\n",
    "            axes[idx].plot(subset['date'], subset[index_label], marker='o', linestyle='', color=color, alpha=0.5)\n",
    "\n",
    "            axes[idx].set_xlabel('Date')\n",
    "            axes[idx].set_ylabel(index_label)\n",
    "            axes[idx].legend()\n",
    "            axes[idx].grid(True)\n",
    "\n",
    "\n",
    "# Configure plot aesthetics\n",
    "def configure_plots(axes, subfolder_name):\n",
    "    for ax in axes:\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=3))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
    "    axes[0].set_title(f'NDVI Over Time - {subfolder_name}')\n",
    "    axes[0].set_ylabel('NDVI Value')\n",
    "    axes[1].set_title(f'NDRE Over Time - {subfolder_name}')\n",
    "    axes[1].set_ylabel('NDRE Value')\n",
    "    axes[2].set_title(f'EVI Over Time - {subfolder_name}')\n",
    "    axes[2].set_ylabel('EVI Value')\n",
    "\n",
    "# Main function \n",
    "def process_shapefiles_and_extract_sentinel_data(base_folder):\n",
    "    suppress_warnings()\n",
    "    base_dir, png_dir, pdf_dir = setup_directories()\n",
    "    subfolders = get_subfolders(base_folder)\n",
    "    all_results = []\n",
    "    result_objects = []\n",
    "\n",
    "    for subfolder in subfolders:\n",
    "        shapefiles = find_shapefiles_in_subfolder(subfolder)\n",
    "        if not shapefiles:\n",
    "            print(f\"No shapefile found in {subfolder}\")\n",
    "            continue\n",
    "        \n",
    "        parcels_geo_df = gpd.read_file(shapefiles[0])\n",
    "        \n",
    "        results = extract_data_for_parcels(parcels_geo_df)\n",
    "        all_results.extend(results)\n",
    "\n",
    "        result_objects.append(results)  \n",
    "\n",
    "        all_pixel_ts_list = compile_time_series_data(result_objects, parcels_geo_df)  \n",
    "\n",
    "        color_mapping = {\n",
    "            '0N': '#66c2a5',\n",
    "            'Nmin': '#fc8d62',\n",
    "            'Standard': '#8da0cb',\n",
    "            '0N_2': '#66c2a5',\n",
    "            'Nmin_2': '#fc8d62',\n",
    "            'Standard_2': '#8da0cb'\n",
    "        }\n",
    "\n",
    "        for subfolder, all_pixel_ts in zip(subfolders, all_pixel_ts_list):\n",
    "            if all_pixel_ts.empty:\n",
    "                continue\n",
    "            \n",
    "            fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(15, 20))\n",
    "            plot_index_graphs(axes, all_pixel_ts, color_mapping)\n",
    "            configure_plots(axes, os.path.basename(subfolder))\n",
    "            plt.tight_layout()\n",
    "            save_plots(os.path.basename(subfolder), fig, png_dir, pdf_dir)\n",
    "            \n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Generation Execution\n",
    "\n",
    "Run the code below to iterate over each subfolder in the specified base directory and create graphs for each set of parcel shapefiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    base_folder_path = setup_directories()[0] / 'field_information' / 'Shapefiles'\n",
    "    results = process_shapefiles_and_extract_sentinel_data(base_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
